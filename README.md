
# Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment

[![ğŸ“„ Paper](https://img.shields.io/badge/arXiv-Paper-red)]()  

[![ğŸ¤— HuggingFace Models](https://img.shields.io/badge/HuggingFace-Models-yellow)](https://huggingface.co/MINT-SJTU/Evo-1/tree/main)  

[![ğŸ“¦ Dataset](https://img.shields.io/badge/HuggingFace-Dataset_MetaWorld-orange)](https://huggingface.co/datasets/MINT-SJTU/Evo1_MetaWorld/tree/main)  





## ğŸ“° News  

- ğŸ—“ï¸ **2025-11-06** â€” Released Meta-World & LIBERO evaluation scripts  
- ğŸ—“ï¸ **2025-11-06** â€” Uploaded model weights to HuggingFace  
- ğŸ—“ï¸ **2025-11-06** â€” Released official code  




## âœ… To-Do List  

- â¬œ Release inference script in xarm6 
- â¬œ Add Evo-1 to the LeRobot framework for SO100   
- â¬œ Release results of all 50 RoboTwin tasks
- â¬œ Release RoboTwin evaluation script  
  



## âš™ï¸ Installation

Prepare the environment for Evo-1

```bash
# Clone this repo
git clone https://github.com/MINT-SJTU/Evo-1.git

# Create a Conda environment
conda create -n Evo1 python=3.10 -y
conda activate Evo1

# Install requirements
cd Evo_1
pip install -r requirements.txt

# You may need to reduce the MAX_JOBS to suit your computer
MAX_JOBS=64 pip install -v flash-attn --no-build-isolation
```

##  Simulation Benchmark

### ğŸ§ª Meta-World Benchmark

### 1ï¸âƒ£ Prepare the environment for Meta-World

```bash
conda create -n metaworld python=3.10 -y
conda activate metaworld
pip install mujoco
pip install metaworld
pip install websockets
pip install opencv-python
pip install packaging
```

### 2ï¸âƒ£ Model Preparation

### ğŸ“¥ 2.1 Download Model Weight

[Meta-World Evaluation Checkpoint](https://huggingface.co/MINT-SJTU/Evo-1/tree/main/Evo1_Simulation_Benchmark_Checkpoints/MetaWorld/Evo1_MetaWorld_checkpoint)


### âœï¸ 2.2 Modify config

Modify checkpoint dir: [Evo1_server.py#L149](Evo_1/scripts/Evo1_server.py#L149)  
(Optional) Modify server port: [Evo1_server.py#L152](Evo_1/scripts/Evo1_server.py#L152)  
(Optional) Modify client port: [mt50_evo1_client_prompt.py#L40](MetaWorld_evaluation/mt50_evo1_client_prompt.py#L40)


### 3ï¸âƒ£ Run Meta-World Evaluation

```bash
# Terminal 1
conda activate Evo1

cd Evo_1

python scripts/Evo1_server.py
```

```bash
# Terminal 2
conda activate metaworld

cd MetaWorld_evaluation

python mt50_evo1_client_prompt.py
```

---

### ğŸ§ª LIBERO Benchmark

### 1ï¸âƒ£ Prepare the environment for LIBERO

```bash
conda create -n libero python=3.8.13 -y

conda activate libero

cd LIBERO_evaluation/

git clone https://github.com/Lifelong-Robot-Learning/LIBERO.git

cd LIBERO

pip install -r requirements.txt

pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113

pip install -e .

pip install websockets
```

### 2ï¸âƒ£ Model Preparation

### ğŸ“¥ 2.1 Download Model Weight

[LIBERO Evaluation Checkpoint](https://huggingface.co/MINT-SJTU/Evo-1/tree/main/Evo1_Simulation_Benchmark_Checkpoints/LIBERO/Evo1_LIBERO_checkpoint)



### âœï¸ 2.2 Modify config
Modify checkpoint dir: [Evo1_server.py#L149](Evo_1/scripts/Evo1_server.py#L149)  
Modify ckpt name: [libero_client_4tasks.py#L24](LIBERO_evaluation/libero_client_4tasks.py#L24)  
(Optional) Modify server port: [Evo1_server.py#L152](Evo_1/scripts/Evo1_server.py#L152)  
(Optional) Modify client port: [libero_client_4tasks.py#L23](LIBERO_evaluation/libero_client_4tasks.py#L23)  


#### 3ï¸âƒ£ Run LIBERO Evaluation

```bash
# Terminal 1
conda activate Evo1

cd Evo_1

python scripts/Evo1_server.py
```

```bash
# Terminal 2
conda activate libero

cd LIBERO_evaluation

python libero_client_4tasks.py
```

## ğŸ§  Training on Your Own Dataset

We support **lerobot v2.1** format, please convert your data to this format.

We use MetaWorld Dataset here as an example.

### ğŸ“¥ 2.1 Download Dataset

```bash
mkdir Evo1_training_dataset/

cd Evo1_training_dataset/

GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/datasets/MINT-SJTU/Evo1_MetaWorld

cd Evo1_MetaWorld/

git lfs pull
```

### âœï¸ 2.2 Modify config

### âœï¸ 2.2.1 Modify config.yaml

You need to modify the [config.yaml](Evo_1/dataset/config.yaml)

This is used to set the dataset path and the camera mapping.

### âœï¸ 2.2 Set the cache path

You need to change the [cache_dir](Evo_1/dataset/lerobot_dataset_pretrain_mp.py#L174)

Set the cache path so the dataset can be loaded from .pkl files next time for faster loading.

### ğŸš€ 3 Start Training

We use the two-stage training paradigm.

### ğŸš€ 3.1 Setup deepspeed
```bash
accelerate config     
```
You can check this [setup guide](deepspeed_steup_example.txt)


### ğŸš€ 3.2 Stage 1

We only train the integration module and action expert in stage 1.   

If you are training with multiple GPU, set --num_processes to the GPU number.  
You need to change the --run_name,--save_dir,--resume_path base on your own config.

```bash
conda activate Evo1

cd Evo_1/

accelerate launch --num_processes 1 --num_machines 1 --deepspeed_config_file ds_config.json scripts/train.py --run_name Evo1_metaworld_stage1 --action_head flowmatching --use_augmentation --lr 1e-5 --dropout 0.2 --weight_decay 1e-3 --batch_size 16 --image_size 448 --max_steps 5000 --log_interval 10 --ckpt_interval 2500 --warmup_steps 1000 --grad_clip_norm 1.0 --num_layers 8 --horizon 50 --finetune_action_head --disable_wandb --vlm_name OpenGVLab/InternVL3-1B --dataset_config_path dataset/config.yaml --per_action_dim 24 --state_dim 24 --save_dir /your/path/checkpoints/stage1
```

### ğŸš€ 3.3 Stage 2
We perform Full-scale training in stage 2.   

```bash
conda activate Evo1

cd Evo_1/

accelerate launch --num_processes 1 --num_machines 1 --deepspeed_config_file ds_config.json scripts/train.py --run_name Evo1_metaworld_stage2 --action_head flowmatching --use_augmentation --lr 1e-5 --dropout 0.2 --weight_decay 1e-3 --batch_size 16 --image_size 448 --max_steps 80000 --log_interval 10 --ckpt_interval 2500 --warmup_steps 1000 --grad_clip_norm 1.0 --num_layers 8 --horizon 50 --finetune_vlm --finetune_action_head --disable_wandb --vlm_name OpenGVLab/InternVL3-1B --dataset_config_path dataset/config.yaml --per_action_dim 24 --state_dim 24 --save_dir /your/path/checkpoints/stage2 --resume --resume_pretrain --resume_path /your/path/checkpoints/stage1/step_5000
```

### ğŸš€ 3.4 (Optional) Resume
If you want to resume the training process, you can use the following command (we use stage 2 as an example):

```bash
accelerate launch --num_processes 1 --num_machines 1 --deepspeed_config_file ds_config.json scripts/train.py --run_name Your_own_name --action_head flowmatching --use_augmentation --lr 1e-5 --dropout 0.2 --weight_decay 1e-3 --batch_size 16 --image_size 448 --max_steps 80000 --log_interval 10 --ckpt_interval 2500 --warmup_steps 1000 --grad_clip_norm 1.0 --num_layers 8 --horizon 50 --finetune_vlm --finetune_action_head --disable_wandb --vlm_name OpenGVLab/InternVL3-1B --dataset_config_path dataset/config.yaml --per_action_dim 24 --state_dim 24 --save_dir /your/path/to/save/the/checkpoints/ --resume  --resume_path /the/checkpoint/path/you/want/to/resume/from/step_20000
```


## ğŸ“š Citatation

## ğŸ“¬ Contact

If you encounter any issues or have suggestions,  
please open an issue or start a discussion on GitHub.  
We sincerely welcome your feedback and contributions.
