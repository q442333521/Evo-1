# Evo-1 原理说明文档

## 1. 项目概述

### 1.1 基本信息
- **项目名称**: Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment
- **论文编号**: arXiv:2511.04555
- **发布时间**: 2025年11月6日
- **研究机构**: 上海交通大学 MINT 实验室
- **模型规模**: 77M 参数（0.77亿参数）

### 1.2 核心贡献
Evo-1 是一个轻量级的视觉-语言-动作（Vision-Language-Action, VLA）模型，主要特点包括：

1. **轻量高效**: 仅使用 77M 参数，大幅降低计算成本和部署难度
2. **语义对齐**: 在不使用机器人数据预训练的情况下，保持强大的语义理解能力
3. **跨模态融合**: 创新性地整合了跨模态扩散Transformer
4. **卓越性能**: 在多个基准测试中超越现有方法
   - Meta-World: 超越前代 12.4%
   - RoboTwin: 超越 6.9%
   - LIBERO: 达到 94.8% 成功率
   - 真实场景: 78% 成功率

## 2. 技术架构

### 2.1 整体架构

Evo-1 采用模块化设计，主要包含三个核心组件：

```
输入 → [视觉-语言嵌入器] → [融合表示] → [动作头] → 动作输出
         (InternVL3)          (整合模块)    (Flow Matching)
```

#### 架构详解：

**输入层**:
- 多视角图像（最多3个视角）
- 图像掩码（标识哪些视角有效）
- 文本指令（任务描述）
- 机器人状态（关节角度等）

**视觉-语言嵌入器**:
- 基于 InternVL3-1B 模型
- 处理多模态输入，生成统一的特征表示
- 支持灵活的微调策略

**动作头**:
- 基于流匹配（Flow Matching）技术
- 使用Transformer架构进行特征整合
- 输出动作序列（action chunks）

### 2.2 核心模块详解

#### 2.2.1 InternVL3 视觉-语言嵌入器

**功能**: 将图像和文本编码为统一的语义特征空间

**特点**:
- 使用预训练的 InternVL3-1B 作为基础模型
- 支持多视角图像输入（最多3个视角）
- 生成 896 维的融合特征向量
- 可选择只返回 CLS token 或完整序列

**工作流程**:
1. 将输入图像调整为 448×448 尺寸
2. 通过视觉编码器提取视觉特征
3. 结合文本指令进行跨模态对齐
4. 输出融合后的特征表示

#### 2.2.2 流匹配动作头（Flow Matching Action Head）

这是 Evo-1 的核心创新之一，采用基于流匹配的生成式方法预测机器人动作。

##### 基本原理

流匹配是一种连续归一化流（Continuous Normalizing Flow）技术，通过学习从噪声分布到目标动作分布的连续变换路径。

**数学表达**:

给定时间 t ∈ [0,1]，中间状态定义为：
```
x_t = (1-t) * noise + t * action_gt
```

模型学习预测速度场（velocity field）：
```
v_θ(x_t, t, context) ≈ (action_gt - noise)
```

在推理时，通过数值积分从随机噪声逐步演化到真实动作：
```
x_{t+dt} = x_t + dt * v_θ(x_t, t, context)
```

##### 架构组成

**1. 动作编码器（MultiEmbodimentActionEncoder）**

- **输入**: 动作序列 (B, H, D)，其中 B=batch size, H=horizon, D=per_action_dim
- **功能**: 将动作序列编码为特征表示
- **组件**:
  - 三层全连接网络 (W1, W2, W3)
  - 正弦位置编码（Sinusoidal Positional Encoding）
  - 支持多具身体系统（Multi-Embodiment）

**位置编码公式**:
```
PE(pos, 2i) = sin(pos / 10000^(2i/dim))
PE(pos, 2i+1) = cos(pos / 10000^(2i/dim))
```

**2. Transformer模块**

- **层数**: 8 层 BasicTransformerBlock
- **注意力头**: 8 个头
- **隐藏维度**: 896 维

每个 Transformer Block 包含：
- 跨注意力机制（Cross-Attention）
  - Query: 动作token
  - Key/Value: 上下文token（视觉-语言特征 + 状态编码）
- 前馈网络（FFN）
- 时间嵌入注入
- 层归一化（LayerNorm）

**3. 状态编码器（State Encoder）**

- **输入**: 机器人当前状态（如关节角度、末端执行器位置）
- **架构**: 两层MLP
  - 输入维度: state_dim (如 24 维)
  - 隐藏维度: 1024 维
  - 输出维度: 896 维
- **功能**: 将机器人状态编码为与视觉-语言特征相同维度的表示

**4. 输出头（MLP Head）**

- **架构**: 类别特定MLP（CategorySpecificMLP）
- **输入**: 池化后的特征 (embed_dim=896)
- **隐藏层**: 1024 维
- **输出**: 动作维度 (action_dim = horizon × per_action_dim)

##### 训练过程

**1. 前向传播**

```python
# 伪代码
def forward(fused_tokens, state, actions_gt):
    # 1. 编码状态
    state_emb = state_encoder(state)
    context = concat([fused_tokens, state_emb])

    # 2. 采样时间 t ~ Beta(2,2)
    t = sample_from_beta(2, 2).clamp(0.02, 0.98)

    # 3. 生成噪声和中间状态
    noise = uniform(-1, 1)
    x_t = (1-t) * noise + t * actions_gt

    # 4. 编码动作和时间
    action_tokens = action_encoder(x_t)
    time_emb = time_positional_encoding(t)

    # 5. Transformer处理
    for block in transformer_blocks:
        action_tokens = block(action_tokens, context, time_emb)

    # 6. 池化和预测
    pooled = pool(action_tokens)
    pred_velocity = mlp_head(pooled)

    return pred_velocity, noise
```

**2. 损失函数**

使用简单的L2损失：
```
Loss = MSE(pred_velocity, actions_gt - noise)
```

这等价于学习从噪声到真实动作的向量场。

##### 推理过程

**Euler积分法**:

```python
def get_action(fused_tokens, state):
    # 1. 初始化为随机噪声
    action = uniform(-1, 1, size=(B, action_dim))

    # 2. 数值积分
    N = 50  # 推理步数
    dt = 1.0 / N

    for i in range(N):
        t = i / N

        # 编码当前动作状态
        action_seq = reshape(action, (B, horizon, per_action_dim))
        action_tokens = action_encoder(action_seq)

        # 添加时间嵌入
        time_emb = time_positional_encoding(t)

        # Transformer处理
        for block in transformer_blocks:
            action_tokens = block(action_tokens, context, time_emb)

        # 预测速度
        pooled = pool(action_tokens)
        velocity = mlp_head(pooled)

        # 更新动作
        action = action + dt * velocity

    return action
```

**关键参数**:
- `num_inference_timesteps`: 推理步数，默认50步
- `dt`: 时间步长 = 1.0 / num_inference_timesteps
- 初始噪声: 从 U(-1, 1) 采样

#### 2.2.3 多具身体系统支持（Multi-Embodiment）

Evo-1 支持同时训练多个机器人平台，通过类别特定层（Category-Specific Layers）实现：

**CategorySpecificLinear**:
```python
# 对于单具身体系统
output = linear(input)

# 对于多具身体系统
output = input @ weight[category_id] + bias[category_id]
```

**优势**:
- 共享视觉-语言理解能力
- 保留各机器人平台的特定动作特性
- 提高泛化能力

### 2.3 动作掩码机制（Action Mask）

支持部分动作维度的控制：

```python
action_mask = [1, 1, 1, 1, 1, 1, 0]  # 前6维有效，第7维无效
masked_action = action * action_mask
```

**应用场景**:
- 固定夹爪状态
- 约束特定关节
- 部分自由度控制

## 3. 训练策略

### 3.1 两阶段训练范式

Evo-1 采用渐进式训练策略，分两个阶段完成模型训练：

#### 阶段一：动作专家训练（5000 steps）

**训练目标**: 学习从视觉-语言特征到动作的映射

**冻结模块**:
- InternVL3 嵌入器（冻结）

**可训练模块**:
- 整合模块（Integration Module）
- 动作头（Action Head）
  - 动作编码器
  - Transformer blocks
  - 状态编码器
  - MLP输出头

**超参数**:
```yaml
learning_rate: 1e-5
batch_size: 16
max_steps: 5000
warmup_steps: 1000
weight_decay: 1e-3
dropout: 0.2
gradient_clip: 1.0
```

**训练命令**:
```bash
accelerate launch --num_processes 1 \
  scripts/train.py \
  --action_head flowmatching \
  --finetune_action_head \
  --lr 1e-5 \
  --batch_size 16 \
  --max_steps 5000
```

#### 阶段二：全模型微调（80000 steps）

**训练目标**: 端到端优化整个模型，增强视觉-语言-动作的协同

**可训练模块**:
- InternVL3 嵌入器（解冻）
- 整合模块
- 动作头

**超参数**:
```yaml
learning_rate: 1e-5
batch_size: 16
max_steps: 80000
warmup_steps: 1000
weight_decay: 1e-3
dropout: 0.2
gradient_clip: 1.0
```

**训练命令**:
```bash
accelerate launch --num_processes 1 \
  scripts/train.py \
  --action_head flowmatching \
  --finetune_vlm \
  --finetune_action_head \
  --lr 1e-5 \
  --batch_size 16 \
  --max_steps 80000 \
  --resume_pretrain \
  --resume_path /path/to/stage1/step_5000
```

### 3.2 数据增强

支持多种图像增强技术：
- 随机裁剪
- 颜色抖动
- 亮度/对比度调整
- 高斯模糊

使用 `--use_augmentation` 标志启用。

### 3.3 分布式训练

支持 DeepSpeed 加速训练：

**配置文件** (`ds_config.json`):
```json
{
  "zero_optimization": {
    "stage": 2
  },
  "fp16": {
    "enabled": true
  },
  "gradient_clipping": 1.0
}
```

**启动命令**:
```bash
accelerate launch \
  --num_processes 8 \
  --num_machines 1 \
  --deepspeed_config_file ds_config.json \
  scripts/train.py [args]
```

## 4. 数据格式

### 4.1 LeRobot v2.1 格式

Evo-1 使用 LeRobot v2.1 数据格式，这是一个标准化的机器人数据格式。

**数据结构**:
```
dataset/
├── episode_0000000/
│   ├── observation.images.image/
│   ├── observation.state/
│   ├── action/
│   └── metadata.json
├── episode_0000001/
└── ...
```

**关键字段**:
- `observation.images.{camera_name}`: 图像数据
- `observation.state`: 机器人状态
- `action`: 动作标签
- `task`: 任务描述

### 4.2 配置文件（config.yaml）

定义数据集路径和视角映射：

```yaml
max_action_dim: 24
max_state_dim: 24
max_views: 3

data_groups:
  # 具身体系统名称
  metaworld_sawyer:
    # 数据集名称
    Evo1_MetaWorld:
      path: /path/to/dataset
      view_map:
        image_1: observation.images.image
        image_2: observation.images.wrist_image
        image_3: observation.images.third_person
```

**多数据集训练**:
```yaml
data_groups:
  libero_franka:
    libero_spatial:
      path: /path/to/spatial
      view_map: {...}
    libero_object:
      path: /path/to/object
      view_map: {...}
```

### 4.3 数据缓存机制

为加速数据加载，Evo-1 实现了 pickle 缓存：

```python
cache_dir = "/path/to/cache"
# 首次加载：解析原始数据并缓存
# 后续加载：直接读取.pkl文件
```

修改缓存路径：`Evo_1/dataset/lerobot_dataset_pretrain_mp.py:174`

## 5. 推理部署

### 5.1 服务器-客户端架构

Evo-1 采用 WebSocket 通信架构：

```
[机器人客户端] <--WebSocket--> [Evo-1服务器]
```

#### 服务器端（Evo1_server.py）

**启动**:
```bash
conda activate Evo1
cd Evo_1
python scripts/Evo1_server.py
```

**关键配置**:
```python
checkpoint_path = "/path/to/checkpoint"
server_port = 8765
model_config = {
    "vlm_name": "OpenGVLab/InternVL3-1B",
    "action_head": "flowmatching",
    "horizon": 50,
    "per_action_dim": 24,
    "state_dim": 24,
}
```

**工作流程**:
1. 加载模型权重
2. 监听 WebSocket 连接
3. 接收观测数据
4. 运行推理
5. 返回动作序列

#### 客户端示例（Evo1_client_xarm6.py）

**观测构造**:
```python
obs = {
    # 图像列表（448x448, RGB）
    "image": [base_image, wrist_image, dummy_image],

    # 图像掩码（1=有效，0=无效）
    "image_mask": [1, 1, 0],

    # 机器人状态
    "state": [q1, q2, q3, q4, q5, q6, gripper, ...],

    # 动作掩码
    "action_mask": [[1, 1, 1, 1, 1, 1, 0, ...]],

    # 任务指令
    "prompt": "pick up the red cube"
}
```

**通信**:
```python
# 发送观测
await ws.send(json.dumps(obs))

# 接收动作
result = await ws.recv()
action_chunk = torch.tensor(json.loads(result))

# action_chunk shape: (1, horizon * per_action_dim)
# 执行第一步动作
action_to_execute = action_chunk[0, :per_action_dim]
```

### 5.2 动作分块执行（Action Chunking）

Evo-1 预测未来 H 步的动作序列（action chunk），但每次只执行第一步：

```python
horizon = 50
per_action_dim = 7

# 预测结果
action_chunk = model.predict()  # shape: (1, 50*7)

# 重塑为序列
action_seq = action_chunk.reshape(50, 7)

# 执行第一步
robot.execute(action_seq[0])

# 下一时刻重新预测
```

**优势**:
- 提高鲁棒性：每步都基于最新观测
- 更好的闭环控制
- 适应动态环境变化

### 5.3 实时性能

**推理速度**:
- GPU: RTX 3090
- 延迟: ~50ms per inference
- 频率: ~20 Hz

**优化建议**:
- 使用 FP16 推理
- 批量处理多个环境
- 减少 `num_inference_timesteps`（50 → 20）

## 6. 评估基准

### 6.1 Meta-World（MT50）

**环境描述**:
- 50 个模拟机械臂任务
- Sawyer 机器人
- 单视角图像

**评估指标**: 成功率（Success Rate）

**运行评估**:
```bash
# Terminal 1: 启动服务器
python scripts/Evo1_server.py

# Terminal 2: 运行评估
cd MetaWorld_evaluation
python mt50_evo1_client_prompt.py
```

**Evo-1 性能**:
- 平均成功率: **超越基线 12.4%**
- 参数量: 77M（远小于其他VLA模型）

### 6.2 LIBERO

**环境描述**:
- 4个任务套件
  - LIBERO-Spatial: 空间推理
  - LIBERO-Object: 物体操作
  - LIBERO-Goal: 目标理解
  - LIBERO-10: 长期任务
- Franka Panda 机器人
- 双视角图像（第三人称 + 腕部）

**评估指标**: 成功率

**运行评估**:
```bash
# Terminal 1: 启动服务器
python scripts/Evo1_server.py

# Terminal 2: 运行评估
cd LIBERO_evaluation
python libero_client_4tasks.py
```

**Evo-1 性能**:
- 成功率: **94.8%**

### 6.3 RoboTwin

**环境描述**:
- 真实场景操作任务
- 多视角图像（3个视角）
- 复杂的长期任务

**Evo-1 性能**:
- 相比基线提升: **6.9%**

### 6.4 真实机器人实验

**平台**: xArm6 机械臂

**任务类型**:
- 抓取
- 放置
- 推动
- 组装

**成功率**: **78%**

## 7. 关键技术细节

### 7.1 Beta分布时间采样

训练时使用 Beta(2,2) 分布采样时间 t：

```python
t = torch.distributions.Beta(2, 2).sample().clamp(0.02, 0.98)
```

**为什么用 Beta(2,2)**:
- 更关注中间时刻 (t≈0.5)
- 避免极端值 (t≈0 或 t≈1)
- 加速收敛

### 7.2 动作归一化

动作值被归一化到 [-1, 1] 范围：

```python
# 训练时
action_normalized = (action - action_min) / (action_max - action_min) * 2 - 1

# 推理时
action = (action_normalized + 1) / 2 * (action_max - action_min) + action_min
```

### 7.3 图像预处理

```python
# 调整大小
image = resize(image, (448, 448))

# 归一化（ImageNet统计）
mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]
image = (image - mean) / std
```

### 7.4 状态维度对齐

不同机器人的状态维度可能不同，通过填充对齐：

```python
max_state_dim = 24
if state.shape[-1] < max_state_dim:
    padding = torch.zeros(..., max_state_dim - state.shape[-1])
    state = torch.cat([state, padding], dim=-1)
```

## 8. 与其他VLA模型的比较

### 8.1 参数规模

| 模型 | 参数量 | 相对大小 |
|------|--------|----------|
| OpenVLA | ~7B | 90.8× |
| RT-2-X | ~55B | 714× |
| Octo | ~93M | 1.2× |
| **Evo-1** | **77M** | **1×** |

### 8.2 性能对比

| 基准 | OpenVLA | Octo | Evo-1 |
|------|---------|------|-------|
| Meta-World | 68.5% | 62.3% | **80.9%** |
| LIBERO | 89.2% | 85.7% | **94.8%** |

### 8.3 核心差异

**Evo-1 的优势**:
1. **更小的模型**: 77M vs 数十亿参数
2. **无需预训练**: 不依赖大规模机器人数据预训练
3. **流匹配架构**: 相比扩散模型更高效
4. **语义对齐**: 保持强大的语言理解能力

**技术对比**:
- **OpenVLA**: 基于大型语言模型（Llama），需要海量预训练
- **Octo**: 基于扩散策略，推理慢
- **RT-2**: 基于视觉-语言模型，参数量巨大
- **Evo-1**: 基于轻量VLM + 流匹配，平衡性能与效率

## 9. 局限性与未来工作

### 9.1 当前局限

1. **单臂操作**: 目前仅支持单臂机器人
2. **固定horizon**: horizon=50 对所有任务固定
3. **图像分辨率**: 448×448 可能损失细节信息
4. **实时性**: 50ms延迟在高速任务中可能不足

### 9.2 未来方向

1. **双臂支持**: 扩展到双臂协同操作
2. **自适应horizon**: 根据任务动态调整预测长度
3. **更高分辨率**: 支持更大图像输入
4. **模型蒸馏**: 进一步压缩模型规模
5. **强化学习**: 结合RL进行在线微调
6. **世界模型**: 集成预测模型增强泛化

## 10. 技术亮点总结

1. **流匹配动作头**:
   - 相比扩散模型更高效
   - 连续归一化流提供平滑动作轨迹
   - 可解释的速度场学习

2. **渐进式训练**:
   - 两阶段训练策略
   - 先学习动作，再端到端优化
   - 加速收敛，提高稳定性

3. **多具身体系统支持**:
   - Category-specific 层设计
   - 共享视觉-语言理解
   - 保留平台特定知识

4. **轻量化设计**:
   - 77M 参数实现强大性能
   - 适合边缘设备部署
   - 降低训练和推理成本

5. **保持语义对齐**:
   - 基于预训练VLM
   - 无需机器人数据预训练
   - 强大的zero-shot泛化能力

## 11. 参考资料

- 论文: [arXiv:2511.04555](https://arxiv.org/abs/2511.04555)
- 代码: [GitHub - MINT-SJTU/Evo-1](https://github.com/MINT-SJTU/Evo-1)
- 模型: [HuggingFace - MINT-SJTU/Evo-1](https://huggingface.co/MINT-SJTU/Evo-1)
- 数据集: [HuggingFace - Evo1_MetaWorld](https://huggingface.co/datasets/MINT-SJTU/Evo1_MetaWorld)
- 项目主页: [https://mint-sjtu.github.io/Evo-1.io/](https://mint-sjtu.github.io/Evo-1.io/)

---

*本文档由上海交通大学 MINT 实验室提供技术支持*
