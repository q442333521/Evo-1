# Evo-1 SO101 使用指南

## 目录

1. [最新更新](#最新更新)
2. [SO101 简介](#so101-简介)
3. [环境安装](#环境安装)
4. [SO101 配置与校准](#so101-配置与校准)
5. [Evo-1 在 LeRobot SO101 上的推理](#evo-1-在-lerobot-so101-上的推理)
6. [数据采集与训练](#数据采集与训练)
7. [社区问答汇总](#社区问答汇总)
8. [常见问题](#常见问题)

---

## 最新更新

### 🎉 2025-11-15 - SO100/SO101 LeRobot 集成发布

Evo-1 推理部分已经集成进 LeRobot 框架，现在可以在 SO100/SO101 机械臂上直接使用！

**主要更新内容：**
- ✅ 添加 Evo-1 策略到 LeRobot 框架（位于 `/so100_evo1/lerobot-main/src/lerobot/policies/evo1/`）
- ✅ 支持 SO100 和 SO101 机械臂的推理
- ✅ 提供完整的环境配置和检查点转换说明
- ✅ 提供示例检查点和配置文件

---

## SO101 简介

### 什么是 SO101？

SO-101 是 HuggingFace LeRobot 的旗舰级低成本机械臂，具有以下特点：

| 特性 | 说明 |
|------|------|
| **6自由度** | 肩部旋转、抬升、肘部、腕部翻转、旋转、夹爪 |
| **主从臂设计** | Leader 臂（示教） + Follower 臂（执行） |
| **低成本** | 使用 Feetech STS3215 舵机 |
| **开源** | 完整的 3D 打印文件和组装说明 |
| **易于集成** | 原生支持 LeRobot 框架 |

### SO101 vs SO100

| 特性 | SO100 | SO101 |
|------|-------|-------|
| **结构设计** | 紧凑型 | 旗舰型，更大工作空间 |
| **电机配置** | 统一齿轮比 | Leader 臂使用不同齿轮比减少操作力 |
| **组装难度** | 需要提前配置电机 | 组装后可配置电机 |
| **推荐用途** | 入门学习 | 研究和生产环境 |

---

## 环境安装

### 1. Evo-1 SO101 环境配置

```bash
# 进入 so100_evo1 目录（SO100 和 SO101 共用相同的环境）
cd Evo-1/so100_evo1/

# 创建 Conda 环境
conda create -n Evo1_SO101 python=3.10 -y
conda activate Evo1_SO101

# 安装 FlashAttention
wget https://ghproxy.net/https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.7cxx11abiTRUE-cp310-cp310-linux_x86_64.whl

pip install flash_attn-2.8.3+cu12torch2.7cxx11abiTRUE-cp310-cp310-linux_x86_64.whl

# 安装 FFmpeg（视频处理）
conda install ffmpeg -c conda-forge

# 安装 LeRobot
cd lerobot-main
pip install -e .

# 安装 Feetech 舵机支持（SO101 必需）
pip install -e ".[feetech]"

# 设置 LEROBOT_HOME 环境变量
# 这个目录需要包含 SO101 的校准文件
export HF_LEROBOT_HOME="/path/to/your/LEROBOT_HOME"

# 安装其他依赖
pip install transformers accelerate timm
```

### 2. 系统要求

**最低配置：**
- **GPU**: NVIDIA RTX 3090 (24GB 显存)
- **RAM**: 16GB
- **系统**: Ubuntu 20.04+ / macOS
- **Python**: 3.10

**推荐配置：**
- **GPU**: NVIDIA A100 (40GB)
- **RAM**: 32GB
- **存储**: 100GB SSD

---

## SO101 配置与校准

### 1. 查找 USB 端口

连接 SO101 的 Leader 臂和 Follower 臂到电脑，然后查找对应的端口：

```bash
lerobot-find-port
```

**Linux 示例输出：**
```
Finding all available ports for the MotorBus.
['/dev/ttyACM0', '/dev/ttyACM1']
Remove the USB cable from your MotorsBus and press Enter when done.

[...拔掉对应的机械臂 USB 并按 Enter...]

The port of this MotorsBus is /dev/ttyACM1
Reconnect the USB cable.
```

**在 Linux 上可能需要授权端口：**
```bash
sudo chmod 666 /dev/ttyACM0
sudo chmod 666 /dev/ttyACM1
```

### 2. 配置电机 ID 和波特率

#### Follower 臂配置

```bash
lerobot-setup-motors \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \  # 替换为你的端口
    --robot.id=my_so101_follower
```

**操作流程：**

1. 按提示依次连接单个电机
2. 从夹爪（motor 6）开始，逐个配置
3. 每个电机会自动设置唯一 ID 和波特率
4. 配置顺序：gripper → wrist_roll → wrist_flex → elbow_flex → shoulder_lift → shoulder_pan

**参考视频：** 查看 [SO101 电机配置视频](https://huggingface.co/docs/lerobot/en/so101#setup-motors-video)

#### Leader 臂配置

```bash
lerobot-setup-motors \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \  # 替换为你的端口
    --teleop.id=my_so101_leader
```

### 3. 校准机械臂

校准是确保 Leader 臂和 Follower 臂位置一致的关键步骤。

#### 校准 Follower 臂

```bash
lerobot-calibrate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_so101_follower
```

**校准步骤：**
1. 将机械臂移动到所有关节处于中间位置
2. 按 `Enter` 确认
3. 依次移动每个关节，通过其完整运动范围
4. 系统会自动记录并保存校准数据

**参考视频：** [SO101 校准视频](https://huggingface.co/docs/lerobot/en/so101#calibration-video)

#### 校准 Leader 臂

```bash
lerobot-calibrate \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_so101_leader
```

---

## Evo-1 在 LeRobot SO101 上的推理

### 1. 检查点准备

训练完成后，需要修改检查点文件以兼容 LeRobot 框架。

#### 1.1 重命名配置文件

```bash
# 进入你的检查点目录
cd /path/to/your/checkpoint/

# 将原始 config.json 重命名
mv config.json model_config.json
```

#### 1.2 创建 LeRobot 兼容的 config.json

根据 `model_config.json` 创建新的 `config.json`，需要修改：

1. **摄像头名称映射**
2. **图像尺寸**
3. **LeRobot 框架所需的字段**

**示例检查点下载：**

```bash
# 下载 Evo-1 官方提供的 SO100 示例检查点（SO101 可参考）
hf download MINT-SJTU/Evo1_SO100 --local-dir /path/to/save/checkpoint/
```

**关键配置项：**

```json
{
  "model_type": "evo1",
  "cameras": {
    "front_env": {
      "width": 640,
      "height": 480,
      "fps": 30
    },
    "side_env": {
      "width": 640,
      "height": 480,
      "fps": 30
    }
  },
  "action_dim": 7,
  "state_dim": 7,
  ...
}
```

### 2. 运行 Evo-1 推理

#### 基本命令格式

```bash
cd Evo-1/so100_evo1

lerobot-record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=your_so101_follower_arm_id \
    --robot.cameras="{
      front: {type: opencv, index_or_path: 2, width: 640, height: 480, fps: 30},
      wrist: {type: opencv, index_or_path: 0, width: 640, height: 480, fps: 30}
    }" \
    --display_data=true \
    --dataset.repo_id=${HF_USER}/eval_evo1 \
    --dataset.single_task="任务描述" \
    --policy.path=/path/to/your/checkpoint/
```

#### 完整示例

```bash
lerobot-record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_so101_follower \
    --robot.cameras="{
      front_env: {type: opencv, index_or_path: 2, width: 640, height: 480, fps: 30},
      side_env: {type: opencv, index_or_path: 0, width: 640, height: 480, fps: 30}
    }" \
    --display_data=true \
    --dataset.repo_id=your_username/eval_evo1_so101 \
    --dataset.single_task="抓取绿色方块并放入绿色盒子中" \
    --policy.path=/home/user/checkpoints/evo1_so101/step_20000/
```

#### 参数说明

| 参数 | 说明 | 示例 |
|------|------|------|
| `--robot.type` | 机械臂类型 | `so101_follower` |
| `--robot.port` | USB 端口 | `/dev/ttyACM0` |
| `--robot.id` | 机械臂 ID（校准时设置的） | `my_so101_follower` |
| `--robot.cameras` | 摄像头配置（YAML 格式） | 见上方示例 |
| `--display_data` | 是否显示实时画面 | `true` / `false` |
| `--dataset.repo_id` | HuggingFace 数据集仓库 | `username/dataset_name` |
| `--dataset.single_task` | 任务描述（中英文均可） | `"Pick up the red cube"` |
| `--policy.path` | Evo-1 检查点路径 | `/path/to/checkpoint/` |

### 3. 摄像头配置

#### 查找摄像头索引

```bash
# Linux 查看可用摄像头
ls /dev/video*

# 测试摄像头
ffplay /dev/video0
ffplay /dev/video2
```

#### 摄像头配置格式

```bash
--robot.cameras="{
  camera_name_1: {
    type: opencv,                # 摄像头类型
    index_or_path: 0,            # 设备索引或路径
    width: 640,                  # 分辨率宽度
    height: 480,                 # 分辨率高度
    fps: 30                      # 帧率
  },
  camera_name_2: {
    type: opencv,
    index_or_path: 2,
    width: 640,
    height: 480,
    fps: 30
  }
}"
```

**注意事项：**
- 摄像头名称需要与训练时的配置一致
- 建议使用固定的 USB 端口避免索引变化
- 如果只有一个摄像头，可以使用相同索引或创建虚拟视角

---

## 数据采集与训练

### 1. 数据采集

使用 LeRobot 的遥操作功能采集数据：

```bash
lerobot-teleoperate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_so101_follower \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_so101_leader \
    --robot.cameras="{
      front_env: {type: opencv, index_or_path: 2, width: 640, height: 480, fps: 30},
      side_env: {type: opencv, index_or_path: 0, width: 640, height: 480, fps: 30}
    }" \
    --dataset.repo_id=your_username/evo1_so101_data \
    --dataset.single_task="任务描述" \
    --num_episodes=100
```

### 2. 数据要求

根据社区反馈：

| 项目 | 建议值 |
|------|--------|
| **单任务数据量** | 100 条轨迹 |
| **单条轨迹时长** | 10-30 秒（根据任务难度） |
| **摄像头视角** | 2 个环境摄像头（固定位置） |
| **数据变化** | 物体位置、盒子位置等 |

**来自项目作者的建议：**
- xArm6 和 SO100 真机实验都使用 100 条数据
- 每条数据 10-30 秒
- SO100 使用两个环境摄像头（腕部摄像头在运输时损坏）
- 采集时保持摄像头位置固定

### 3. 训练模型

将数据转换为 LeRobot v2.1 格式后，使用 Evo-1 两阶段训练：

#### 阶段一：动作专家训练（5,000 步）

```bash
cd Evo-1/Evo_1/

accelerate launch \
  --num_processes 1 \
  --deepspeed_config_file ds_config.json \
  scripts/train.py \
  --run_name evo1_so101_stage1 \
  --action_head flowmatching \
  --use_augmentation \
  --lr 1e-5 \
  --batch_size 16 \
  --max_steps 5000 \
  --finetune_action_head \
  --per_action_dim 7 \
  --state_dim 7 \
  --dataset_config_path dataset/config_so101.yaml \
  --save_dir /path/to/checkpoints/stage1
```

#### 阶段二：全模型微调（80,000 步）

```bash
accelerate launch \
  --num_processes 1 \
  --deepspeed_config_file ds_config.json \
  scripts/train.py \
  --run_name evo1_so101_stage2 \
  --action_head flowmatching \
  --use_augmentation \
  --lr 1e-5 \
  --batch_size 16 \
  --max_steps 80000 \
  --finetune_vlm \
  --finetune_action_head \
  --per_action_dim 7 \
  --state_dim 7 \
  --dataset_config_path dataset/config_so101.yaml \
  --save_dir /path/to/checkpoints/stage2 \
  --resume \
  --resume_pretrain \
  --resume_path /path/to/checkpoints/stage1/step_5000
```

**显存要求：**
- 24GB GPU（RTX 3090）：可以训练，但 batch_size 需要设置较小（如 8）
- 建议使用梯度累积：`--gradient_accumulation_steps 2`

---

## 社区问答汇总

### Q1: SO101 上复现需要多少数据？

**A:** 根据项目作者 @林涛 的回复：

- **Evo-1** 在 xArm6 和 SO100 真机上都使用了 **100 条轨迹**
- 每条轨迹时长 **10-30 秒**（根据任务难度）
- **两个环境摄像头**（SO100 腕部摄像头损坏）
- 数据变量主要是**目标物体位置和盒子位置**

### Q2: Evo-1 支持异构机械臂吗？

**A:** 支持！

Evo-1 使用 **Server-Client 推理方式**，可以根据你的机械臂适配 Client 代码：

1. 服务器端运行 Evo-1 模型
2. 客户端适配你的机械臂控制接口
3. 通过 WebSocket 通信

可以参考 `Evo_1/scripts/Evo1_client_xarm6.py` 作为模板。

### Q3: 需要从头训练还是可以 Fine-tune？

**A:** Evo-1 使用**两阶段从头训练**方法：

- **Stage 1**: 仅训练动作头
- **Stage 2**: 全模型微调

项目作者建议从头训练以获得最佳性能。

### Q4: 训练显存需求？

**A:**
- **24GB GPU (RTX 3090)**: 可以训练，但需要减小 batch_size
- 建议使用梯度累积来模拟大 batch_size
- Stage 2 全模型微调显存需求较高

### Q5: 单任务、单场景的泛化性如何？

**A:** 根据论文 4.3 节 Generalization Experiments：

- **单任务训练**可以在相同任务的不同场景下泛化
- 对于 **Pick & Place** 任务，可以泛化到不同物体位置
- 更强的泛化需要**多场景多任务训练**
- xArm6 上的实验展示了良好的泛化能力（详见论文）

### Q6: 可以同时训练多个场景或机械臂吗？

**A:** 可以！

在 `dataset/config.yaml` 中配置：

```yaml
data_groups:
  so101_robot_1:
    dataset_1:
      path: /path/to/data1
      view_map: {...}
    dataset_2:
      path: /path/to/data2
      view_map: {...}

  so101_robot_2:
    dataset_3:
      path: /path/to/data3
      view_map: {...}
```

可能需要修改配置文件以支持不同机械臂。

---

## 常见问题

### 问题 1: 找不到 USB 端口

**解决方法：**

```bash
# Linux - 授权端口
sudo chmod 666 /dev/ttyACM*

# 查看所有串口设备
ls -l /dev/ttyACM*
ls -l /dev/ttyUSB*

# 使用 lerobot-find-port 工具
lerobot-find-port
```

### 问题 2: 电机配置失败

**可能原因：**
1. 电源未连接
2. USB 线缆问题
3. 电机已被其他程序占用
4. Waveshare 控制板跳线设置错误（应设置在 B 通道 USB）

**解决方法：**
```bash
# 检查设备是否被占用
lsof | grep ttyACM

# 重新插拔 USB 和电源
# 确保只连接一个电机
```

### 问题 3: 校准数据丢失

**原因：** 校准数据保存在 `$HF_LEROBOT_HOME` 目录下

**解决方法：**
```bash
# 设置环境变量（永久保存）
echo 'export HF_LEROBOT_HOME="/path/to/your/lerobot_home"' >> ~/.bashrc
source ~/.bashrc

# 检查校准文件
ls $HF_LEROBOT_HOME/.cache/calibration/
```

### 问题 4: 摄像头无法识别

**解决方法：**

```bash
# 查看可用摄像头
v4l2-ctl --list-devices

# 测试摄像头
ffplay /dev/video0

# 检查权限
sudo chmod 666 /dev/video*
```

### 问题 5: 推理时机械臂抖动

**可能原因：**
1. 动作归一化不正确
2. 检查点配置与实际机械臂不匹配
3. 模型未充分训练

**解决方法：**
1. 检查 `config.json` 中的动作维度配置
2. 确认摄像头名称映射正确
3. 增加训练步数或数据量

### 问题 6: LeRobot 版本兼容性

**注意：** Evo-1 使用 **LeRobot v2.1** 格式

```bash
# 检查 LeRobot 版本
pip show lerobot

# 使用项目提供的 LeRobot 版本
cd Evo-1/so100_evo1/lerobot-main
pip install -e ".[feetech]"
```

---

## 资源链接

### 官方资源

- **Evo-1 论文**: [arXiv:2511.04555](https://arxiv.org/abs/2511.04555)
- **Evo-1 GitHub**: [https://github.com/MINT-SJTU/Evo-1](https://github.com/MINT-SJTU/Evo-1)
- **Evo-1 模型**: [HuggingFace - MINT-SJTU/Evo-1](https://huggingface.co/MINT-SJTU/Evo-1)
- **SO100 示例检查点**: [HuggingFace - MINT-SJTU/Evo1_SO100](https://huggingface.co/MINT-SJTU/Evo1_SO100)

### LeRobot 文档

- **SO101 组装指南**: [LeRobot SO101 文档](https://huggingface.co/docs/lerobot/en/so101)
- **SO100 组装指南**: [LeRobot SO100 文档](https://huggingface.co/docs/lerobot/en/so100)
- **LeRobot 官方文档**: [https://huggingface.co/docs/lerobot](https://huggingface.co/docs/lerobot)

### SO101 硬件

- **SO-ARM100 GitHub**: [https://github.com/TheRobotStudio/SO-ARM100](https://github.com/TheRobotStudio/SO-ARM100)
- **物料清单 (BOM)**: 见 SO-ARM100 仓库
- **3D 打印文件**: 见 SO-ARM100 仓库

---

## 快速开始检查清单

- [ ] 安装 Conda 环境和依赖包
- [ ] 下载并安装 FlashAttention
- [ ] 安装 LeRobot 和 Feetech SDK
- [ ] 设置 `HF_LEROBOT_HOME` 环境变量
- [ ] 查找 SO101 的 USB 端口
- [ ] 配置 Follower 臂电机 ID
- [ ] 配置 Leader 臂电机 ID
- [ ] 校准 Follower 臂
- [ ] 校准 Leader 臂
- [ ] 配置摄像头
- [ ] 采集训练数据（100 条轨迹）
- [ ] 训练 Evo-1 模型（Stage 1 + Stage 2）
- [ ] 修改检查点配置文件
- [ ] 运行推理测试

---

## 联系与支持

如果遇到问题或有建议：

- **GitHub Issues**: [https://github.com/MINT-SJTU/Evo-1/issues](https://github.com/MINT-SJTU/Evo-1/issues)
- **LeRobot Discord**: [https://discord.com/invite/s3KuuzsPFb](https://discord.com/invite/s3KuuzsPFb)
- **微信群**: 扫描项目 README 中的二维码加入

---

*本文档最后更新: 2025-11-15*
